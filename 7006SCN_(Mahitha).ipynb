{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# MOUNT GOOGLE DRIVE + CREATE FOLDERS"
      ],
      "metadata": {
        "id": "xo1eUjV5bnbW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import os\n",
        "BASE_PATH = \"/content/drive/MyDrive/7006SCN_project\"\n",
        "os.makedirs(f\"{BASE_PATH}/data\", exist_ok=True)\n",
        "os.makedirs(f\"{BASE_PATH}/models\", exist_ok=True)\n",
        "os.makedirs(f\"{BASE_PATH}/metrics\", exist_ok=True)\n",
        "\n",
        "print(\"BASE_PATH:\", BASE_PATH)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MOeXrlzxblx0",
        "outputId": "a7a35e94-e8ca-49f4-ae2c-7a23ac2c052d"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "BASE_PATH: /content/drive/MyDrive/7006SCN_project\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# INSTALL LIBRARIES"
      ],
      "metadata": {
        "id": "_6YvpQisbtLN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip -q install datasets pyspark pyarrow"
      ],
      "metadata": {
        "id": "Ojjn72UHbrVW"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LOAD Data"
      ],
      "metadata": {
        "id": "rttdTplGby6T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "ds_k = load_dataset(\"sebastiandizon/genius-song-lyrics\", split=\"train[:100000]\")\n",
        "\n",
        "df = ds_k.to_pandas()\n",
        "print(\"Loaded df shape:\", df.shape)\n",
        "print(df.columns)\n",
        "\n",
        "# Keep only useful columns\n",
        "keep_cols = [\"title\", \"tag\", \"artist\", \"year\", \"views\", \"lyrics\"]\n",
        "df = df[keep_cols].dropna(subset=[\"tag\", \"lyrics\"])\n",
        "\n",
        "# Optional: reduce noise (remove empty lyrics)\n",
        "df = df[df[\"lyrics\"].astype(str).str.len() > 0]\n",
        "\n",
        "print(\"After cleaning:\", df.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q1zlKTXZbw5Z",
        "outputId": "dcb5f591-3f63-4721-bfbe-7a7743ff5e08"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded df shape: (100000, 11)\n",
            "Index(['title', 'tag', 'artist', 'year', 'views', 'features', 'lyrics', 'id',\n",
            "       'language_cld3', 'language_ft', 'language'],\n",
            "      dtype='object')\n",
            "After cleaning: (100000, 6)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SAVE SAMPLE DATA TO DRIVE (PARQUET + CSV)"
      ],
      "metadata": {
        "id": "pRnak92Ib_Ox"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "raw_parquet = f\"{BASE_PATH}/data/raw_sample_100k.parquet\"\n",
        "raw_csv = f\"{BASE_PATH}/data/raw_sample_100k.csv\"\n",
        "\n",
        "df.to_parquet(raw_parquet, index=False)\n",
        "df.to_csv(raw_csv, index=False)\n",
        "\n",
        "print(\"Saved Parquet:\", raw_parquet)\n",
        "print(\"Saved CSV:\", raw_csv)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RhJKt15Lb_As",
        "outputId": "294b3e9c-9fb8-4637-ac9f-18cb8de96632"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saved Parquet: /content/drive/MyDrive/7006SCN_project/data/raw_sample_100k.parquet\n",
            "Saved CSV: /content/drive/MyDrive/7006SCN_project/data/raw_sample_100k.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# START SPARK (SAFE CONFIG FOR COLAB)"
      ],
      "metadata": {
        "id": "Z_ledkUOcGGS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = SparkSession.builder \\\n",
        "    .appName(\"7006SCN_GeniusLyrics_100k\") \\\n",
        "    .config(\"spark.driver.memory\", \"8g\") \\\n",
        "    .config(\"spark.executor.memory\", \"4g\") \\\n",
        "    .config(\"spark.sql.shuffle.partitions\", \"200\") \\\n",
        "    .getOrCreate()\n",
        "\n",
        "print(\"Spark started\")\n",
        "\n",
        "# Load parquet into Spark\n",
        "sdf = spark.read.parquet(raw_parquet)\n",
        "print(\"Spark rows:\", sdf.count())\n",
        "sdf.printSchema()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O7EoH_WUb7-f",
        "outputId": "a9eb906e-4da0-4e4b-8a5b-2b456a71bdb5"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Spark started\n",
            "Spark rows: 100000\n",
            "root\n",
            " |-- title: string (nullable = true)\n",
            " |-- tag: string (nullable = true)\n",
            " |-- artist: string (nullable = true)\n",
            " |-- year: long (nullable = true)\n",
            " |-- views: long (nullable = true)\n",
            " |-- lyrics: string (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TRAIN/TEST SPLIT"
      ],
      "metadata": {
        "id": "R1pUAu-McJPI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql.functions import col\n",
        "\n",
        "sdf = sdf.filter(col(\"lyrics\").isNotNull() & col(\"tag\").isNotNull())\n",
        "train_df, test_df = sdf.randomSplit([0.8, 0.2], seed=42)\n",
        "\n",
        "print(\"Train rows:\", train_df.count())\n",
        "print(\"Test rows:\", test_df.count())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RBM7pgqNcMgK",
        "outputId": "03f8d933-a787-4f45-c5df-3b8e7b82409a"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train rows: 79901\n",
            "Test rows: 20099\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# FEATURE PIPELINE: TAG -> label, LYRICS -> TFIDF FEATURES"
      ],
      "metadata": {
        "id": "SQa0SWY4cOOo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.feature import (\n",
        "    StringIndexer, RegexTokenizer, StopWordsRemover,\n",
        "    HashingTF, IDF\n",
        ")\n",
        "from pyspark.ml import Pipeline\n",
        "\n",
        "label_indexer = StringIndexer(inputCol=\"tag\", outputCol=\"label\", handleInvalid=\"skip\")\n",
        "\n",
        "tokenizer = RegexTokenizer(inputCol=\"lyrics\", outputCol=\"tokens\", pattern=\"\\\\W+\")\n",
        "remover = StopWordsRemover(inputCol=\"tokens\", outputCol=\"filtered_tokens\")\n",
        "\n",
        "# HashingTF is scalable and memory-friendly for text\n",
        "tf = HashingTF(inputCol=\"filtered_tokens\", outputCol=\"rawFeatures\", numFeatures=1 << 18)\n",
        "idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n"
      ],
      "metadata": {
        "id": "m92QBLl4cQmW"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DEFINE 4 REAL ML MODELS (MLlib)"
      ],
      "metadata": {
        "id": "372U0h3NcSs4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.classification import (LogisticRegression, NaiveBayes, RandomForestClassifier)\n",
        "from pyspark.ml.classification import LinearSVC, OneVsRest\n",
        "\n",
        "models = {\n",
        "    \"LogisticRegression\": LogisticRegression(featuresCol=\"features\", labelCol=\"label\", maxIter=20),\n",
        "    \"NaiveBayes\": NaiveBayes(featuresCol=\"features\", labelCol=\"label\", smoothing=1.0),\n",
        "    \"RandomForest\": RandomForestClassifier(featuresCol=\"features\", labelCol=\"label\", numTrees=100, maxDepth=10),\n",
        "\n",
        "    \"OneVsRest_LinearSVC\": OneVsRest(\n",
        "        classifier=LinearSVC(featuresCol=\"features\", labelCol=\"label\", maxIter=30, regParam=0.1),\n",
        "        labelCol=\"label\",\n",
        "        featuresCol=\"features\"\n",
        "    )\n",
        "}"
      ],
      "metadata": {
        "id": "8wHK4fM7cVvO"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# TRAIN + EVALUATE + SAVE MODELS + SAVE METRICS CSV"
      ],
      "metadata": {
        "id": "3nf_ts7DcXtZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
        "import time\n",
        "\n",
        "evaluator_acc = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
        "evaluator_f1  = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"f1\")\n",
        "evaluator_wpr = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"weightedPrecision\")\n",
        "evaluator_wre = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"weightedRecall\")\n",
        "\n",
        "results = []"
      ],
      "metadata": {
        "id": "TN95n-YjceFG"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Cache train/test for repeated reading across models\n",
        "train_df.cache()\n",
        "test_df.cache()\n",
        "\n",
        "for name, clf in models.items():\n",
        "    print(f\"\\n==============================\")\n",
        "    print(f\"Training: {name}\")\n",
        "    print(f\"==============================\")\n",
        "\n",
        "    pipeline = Pipeline(stages=[label_indexer, tokenizer, remover, tf, idf, clf])\n",
        "\n",
        "    start = time.time()\n",
        "    fitted = pipeline.fit(train_df)\n",
        "    train_time = time.time() - start\n",
        "\n",
        "    preds = fitted.transform(test_df)\n",
        "\n",
        "    acc = evaluator_acc.evaluate(preds)\n",
        "    f1  = evaluator_f1.evaluate(preds)\n",
        "    wpr = evaluator_wpr.evaluate(preds)\n",
        "    wre = evaluator_wre.evaluate(preds)\n",
        "\n",
        "    # Save Spark model to Drive\n",
        "    model_path = f\"{BASE_PATH}/models/{name}\"\n",
        "    fitted.write().overwrite().save(model_path)\n",
        "\n",
        "    # Collect metrics\n",
        "    results.append({\n",
        "        \"model\": name,\n",
        "        \"accuracy\": float(acc),\n",
        "        \"f1\": float(f1),\n",
        "        \"weighted_precision\": float(wpr),\n",
        "        \"weighted_recall\": float(wre),\n",
        "        \"train_seconds\": float(train_time),\n",
        "        \"train_rows\": int(train_df.count()),\n",
        "        \"test_rows\": int(test_df.count())\n",
        "    })\n",
        "\n",
        "    print(f\"{name} metrics:\")\n",
        "    print(f\"  accuracy={acc:.4f}\")\n",
        "    print(f\"  f1={f1:.4f}\")\n",
        "    print(f\"  weighted_precision={wpr:.4f}\")\n",
        "    print(f\"  weighted_recall={wre:.4f}\")\n",
        "    print(f\"  train_seconds={train_time:.1f}\")\n",
        "    print(\"Saved model:\", model_path)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZIbUsPP9ck7G",
        "outputId": "ad2be1cc-9d8c-4928-a4b7-090653212eee"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==============================\n",
            "Training: LogisticRegression\n",
            "==============================\n",
            "LogisticRegression metrics:\n",
            "  accuracy=0.8075\n",
            "  f1=0.8385\n",
            "  weighted_precision=0.8857\n",
            "  weighted_recall=0.8075\n",
            "  train_seconds=307.5\n",
            "Saved model: /content/drive/MyDrive/7006SCN_project/models/LogisticRegression\n",
            "\n",
            "==============================\n",
            "Training: NaiveBayes\n",
            "==============================\n",
            "NaiveBayes metrics:\n",
            "  accuracy=0.8624\n",
            "  f1=0.8832\n",
            "  weighted_precision=0.9162\n",
            "  weighted_recall=0.8624\n",
            "  train_seconds=114.4\n",
            "Saved model: /content/drive/MyDrive/7006SCN_project/models/NaiveBayes\n",
            "\n",
            "==============================\n",
            "Training: RandomForest\n",
            "==============================\n",
            "RandomForest metrics:\n",
            "  accuracy=0.8871\n",
            "  f1=0.8368\n",
            "  weighted_precision=0.8201\n",
            "  weighted_recall=0.8871\n",
            "  train_seconds=4377.8\n",
            "Saved model: /content/drive/MyDrive/7006SCN_project/models/RandomForest\n",
            "\n",
            "==============================\n",
            "Training: OneVsRest_LinearSVC\n",
            "==============================\n",
            "OneVsRest_LinearSVC metrics:\n",
            "  accuracy=0.9059\n",
            "  f1=0.8938\n",
            "  weighted_precision=0.8886\n",
            "  weighted_recall=0.9059\n",
            "  train_seconds=352.0\n",
            "Saved model: /content/drive/MyDrive/7006SCN_project/models/OneVsRest_LinearSVC\n"
          ]
        }
      ]
    }
  ]
}